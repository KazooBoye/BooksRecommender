{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b9a10f6",
   "metadata": {},
   "source": [
    "# Book Recommendation Dataset - Data Exploration\n",
    "\n",
    "This notebook explores the book recommendation dataset to understand its structure, distribution, and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f6f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510966dd",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "try:\n",
    "    train_df = pd.read_csv('../data/processed/train.csv')\n",
    "    val_df = pd.read_csv('../data/processed/val.csv')\n",
    "    test_df = pd.read_csv('../data/processed/test.csv')\n",
    "    \n",
    "    print(f\"Training set: {len(train_df)} books\")\n",
    "    print(f\"Validation set: {len(val_df)} books\")\n",
    "    print(f\"Test set: {len(test_df)} books\")\n",
    "    print(f\"Total: {len(train_df) + len(val_df) + len(test_df)} books\")\n",
    "    \n",
    "    # Combine for exploration\n",
    "    df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "    print(f\"\\nCombined dataset shape: {df.shape}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Processed data not found. Please run data preprocessing first.\")\n",
    "    # Try loading raw data instead\n",
    "    try:\n",
    "        df = pd.read_csv('../data/raw/goodreads/books.csv')\n",
    "        print(f\"Loaded raw data with {len(df)} books\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No data found. Please collect data first.\")\n",
    "        df = pd.DataFrame()  # Empty dataframe for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "if not df.empty:\n",
    "    print(\"Dataset Info:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nColumn names:\")\n",
    "    print(df.columns.tolist())\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d72c986",
   "metadata": {},
   "source": [
    "## 2. Missing Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Missing data analysis\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percentage = (missing_data / len(df)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_data,\n",
    "        'Percentage': missing_percentage\n",
    "    }).sort_values('Percentage', ascending=False)\n",
    "    \n",
    "    print(\"Missing Data Summary:\")\n",
    "    display(missing_df[missing_df['Missing Count'] > 0])\n",
    "    \n",
    "    # Visualize missing data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_cols = missing_df[missing_df['Missing Count'] > 0]\n",
    "    if not missing_cols.empty:\n",
    "        plt.bar(missing_cols.index, missing_cols['Percentage'])\n",
    "        plt.title('Missing Data Percentage by Column')\n",
    "        plt.xlabel('Columns')\n",
    "        plt.ylabel('Missing Percentage (%)')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No missing data found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205949d3",
   "metadata": {},
   "source": [
    "## 3. Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c0796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Analyze text length for key fields\n",
    "    text_columns = ['title', 'author', 'description']\n",
    "    \n",
    "    for col in text_columns:\n",
    "        if col in df.columns:\n",
    "            # Calculate text lengths\n",
    "            lengths = df[col].dropna().astype(str).str.len()\n",
    "            \n",
    "            print(f\"\\n{col.upper()} Length Statistics:\")\n",
    "            print(f\"Mean: {lengths.mean():.1f}\")\n",
    "            print(f\"Median: {lengths.median():.1f}\")\n",
    "            print(f\"Min: {lengths.min()}\")\n",
    "            print(f\"Max: {lengths.max()}\")\n",
    "            print(f\"95th percentile: {lengths.quantile(0.95):.1f}\")\n",
    "    \n",
    "    # Visualize text lengths\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Text Length Distributions', fontsize=16)\n",
    "    \n",
    "    for i, col in enumerate(text_columns):\n",
    "        if col in df.columns:\n",
    "            lengths = df[col].dropna().astype(str).str.len()\n",
    "            \n",
    "            # Histogram\n",
    "            ax = axes[i//2, i%2]\n",
    "            ax.hist(lengths, bins=50, alpha=0.7, edgecolor='black')\n",
    "            ax.set_title(f'{col.capitalize()} Length Distribution')\n",
    "            ax.set_xlabel('Character Count')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            \n",
    "            # Add statistics as text\n",
    "            ax.axvline(lengths.mean(), color='red', linestyle='--', label=f'Mean: {lengths.mean():.0f}')\n",
    "            ax.axvline(lengths.median(), color='green', linestyle='--', label=f'Median: {lengths.median():.0f}')\n",
    "            ax.legend()\n",
    "    \n",
    "    # Remove empty subplot\n",
    "    if len(text_columns) < 4:\n",
    "        axes[1, 1].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563dbd19",
   "metadata": {},
   "source": [
    "## 4. Genre Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf34048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'genre' in df.columns:\n",
    "    # Analyze genres\n",
    "    genres_series = df['genre'].dropna()\n",
    "    \n",
    "    # Split genres and count\n",
    "    all_genres = []\n",
    "    for genre_str in genres_series:\n",
    "        if pd.notna(genre_str):\n",
    "            genres = [g.strip().lower() for g in str(genre_str).split(',')]\n",
    "            all_genres.extend(genres)\n",
    "    \n",
    "    genre_counts = Counter(all_genres)\n",
    "    \n",
    "    print(f\"Total unique genres: {len(genre_counts)}\")\n",
    "    print(f\"\\nTop 20 most common genres:\")\n",
    "    \n",
    "    top_genres = genre_counts.most_common(20)\n",
    "    for genre, count in top_genres:\n",
    "        print(f\"{genre:20s}: {count:4d} ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Visualize top genres\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    genres, counts = zip(*top_genres)\n",
    "    plt.barh(range(len(genres)), counts)\n",
    "    plt.yticks(range(len(genres)), genres)\n",
    "    plt.xlabel('Number of Books')\n",
    "    plt.title('Top 20 Most Common Genres')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Genre distribution per book\n",
    "    genres_per_book = []\n",
    "    for genre_str in genres_series:\n",
    "        if pd.notna(genre_str):\n",
    "            num_genres = len([g.strip() for g in str(genre_str).split(',')])\n",
    "            genres_per_book.append(num_genres)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(genres_per_book, bins=range(1, max(genres_per_book)+2), alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Number of Genres per Book')\n",
    "    plt.ylabel('Number of Books')\n",
    "    plt.title('Distribution of Number of Genres per Book')\n",
    "    plt.xticks(range(1, max(genres_per_book)+1))\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nAverage genres per book: {np.mean(genres_per_book):.2f}\")\n",
    "    print(f\"Median genres per book: {np.median(genres_per_book):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fcc4dd",
   "metadata": {},
   "source": [
    "## 5. Publication Year Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dfca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'publication_year' in df.columns:\n",
    "    # Clean and analyze publication years\n",
    "    pub_years = pd.to_numeric(df['publication_year'], errors='coerce').dropna()\n",
    "    \n",
    "    print(f\"Publication Year Statistics:\")\n",
    "    print(f\"Earliest: {pub_years.min():.0f}\")\n",
    "    print(f\"Latest: {pub_years.max():.0f}\")\n",
    "    print(f\"Mean: {pub_years.mean():.1f}\")\n",
    "    print(f\"Median: {pub_years.median():.0f}\")\n",
    "    \n",
    "    # Filter reasonable years (remove outliers)\n",
    "    reasonable_years = pub_years[(pub_years >= 1800) & (pub_years <= 2025)]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Histogram of publication years\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.hist(reasonable_years, bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Publication Year')\n",
    "    plt.ylabel('Number of Books')\n",
    "    plt.title('Distribution of Publication Years')\n",
    "    \n",
    "    # Books by decade\n",
    "    plt.subplot(2, 2, 2)\n",
    "    decades = (reasonable_years // 10) * 10\n",
    "    decade_counts = decades.value_counts().sort_index()\n",
    "    plt.bar(decade_counts.index, decade_counts.values, width=8, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Decade')\n",
    "    plt.ylabel('Number of Books')\n",
    "    plt.title('Books Published by Decade')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Recent years (2000+)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    recent_years = reasonable_years[reasonable_years >= 2000]\n",
    "    if len(recent_years) > 0:\n",
    "        year_counts = recent_years.value_counts().sort_index()\n",
    "        plt.plot(year_counts.index, year_counts.values, marker='o', linewidth=2)\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Number of Books')\n",
    "        plt.title('Books Published 2000-Present')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    # Box plot by decade\n",
    "    plt.subplot(2, 2, 4)\n",
    "    decade_labels = sorted(decades.unique())\n",
    "    decade_data = [reasonable_years[decades == decade] for decade in decade_labels]\n",
    "    plt.boxplot(decade_data, labels=[f\"{int(d)}s\" for d in decade_labels])\n",
    "    plt.xlabel('Decade')\n",
    "    plt.ylabel('Publication Year')\n",
    "    plt.title('Publication Year Distribution by Decade')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426dfeb0",
   "metadata": {},
   "source": [
    "## 6. Author Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3978d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'author' in df.columns:\n",
    "    # Analyze authors\n",
    "    authors = df['author'].dropna()\n",
    "    \n",
    "    print(f\"Total unique authors: {authors.nunique()}\")\n",
    "    print(f\"Total books with author info: {len(authors)}\")\n",
    "    \n",
    "    # Most prolific authors\n",
    "    author_counts = authors.value_counts()\n",
    "    \n",
    "    print(f\"\\nTop 15 most prolific authors:\")\n",
    "    for author, count in author_counts.head(15).items():\n",
    "        print(f\"{author:30s}: {count:3d} books\")\n",
    "    \n",
    "    # Distribution of books per author\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(author_counts.values, bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Number of Books per Author')\n",
    "    plt.ylabel('Number of Authors')\n",
    "    plt.title('Distribution of Books per Author')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    top_authors = author_counts.head(20)\n",
    "    plt.barh(range(len(top_authors)), top_authors.values)\n",
    "    plt.yticks(range(len(top_authors)), [name[:20] + '...' if len(name) > 20 else name for name in top_authors.index])\n",
    "    plt.xlabel('Number of Books')\n",
    "    plt.title('Top 20 Most Prolific Authors')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    # Single book authors vs multi-book authors\n",
    "    single_book = (author_counts == 1).sum()\n",
    "    multi_book = (author_counts > 1).sum()\n",
    "    \n",
    "    plt.pie([single_book, multi_book], \n",
    "            labels=[f'Single Book\\n({single_book})', f'Multiple Books\\n({multi_book})'],\n",
    "            autopct='%1.1f%%')\n",
    "    plt.title('Authors by Number of Books')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nAuthors with only one book: {single_book} ({single_book/len(author_counts)*100:.1f}%)\")\n",
    "    print(f\"Authors with multiple books: {multi_book} ({multi_book/len(author_counts)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da96708",
   "metadata": {},
   "source": [
    "## 7. Word Cloud and Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cbd96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to create word clouds (optional, requires wordcloud package)\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    if not df.empty and 'description' in df.columns:\n",
    "        # Combine all descriptions\n",
    "        all_descriptions = ' '.join(df['description'].dropna().astype(str))\n",
    "        \n",
    "        # Create word cloud\n",
    "        wordcloud = WordCloud(width=800, height=400, \n",
    "                             background_color='white',\n",
    "                             max_words=100,\n",
    "                             colormap='viridis').generate(all_descriptions)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('Word Cloud of Book Descriptions', fontsize=16)\n",
    "        plt.show()\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"WordCloud package not available. Skipping word cloud generation.\")\n",
    "    print(\"Install with: pip install wordcloud\")\n",
    "\n",
    "# Basic text analysis without wordcloud\n",
    "if not df.empty and 'description' in df.columns:\n",
    "    # Most common words in descriptions\n",
    "    all_text = ' '.join(df['description'].dropna().astype(str).str.lower())\n",
    "    \n",
    "    # Simple word extraction (remove common stop words)\n",
    "    stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', \n",
    "                     'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', \n",
    "                     'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that',\n",
    "                     'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'])\n",
    "    \n",
    "    words = re.findall(r'\\b[a-z]{3,}\\b', all_text)  # Words with 3+ letters\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    word_counts = Counter(filtered_words)\n",
    "    \n",
    "    print(f\"\\nTop 20 most common words in descriptions:\")\n",
    "    for word, count in word_counts.most_common(20):\n",
    "        print(f\"{word:15s}: {count:5d}\")\n",
    "    \n",
    "    # Visualize top words\n",
    "    top_words = word_counts.most_common(15)\n",
    "    words, counts = zip(*top_words)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(range(len(words)), counts)\n",
    "    plt.yticks(range(len(words)), words)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.title('Top 15 Most Common Words in Book Descriptions')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6568e98",
   "metadata": {},
   "source": [
    "## 8. Rating Analysis (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa2690",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'rating' in df.columns:\n",
    "    ratings = pd.to_numeric(df['rating'], errors='coerce').dropna()\n",
    "    \n",
    "    if len(ratings) > 0:\n",
    "        print(f\"Rating Statistics:\")\n",
    "        print(f\"Mean: {ratings.mean():.2f}\")\n",
    "        print(f\"Median: {ratings.median():.2f}\")\n",
    "        print(f\"Min: {ratings.min():.2f}\")\n",
    "        print(f\"Max: {ratings.max():.2f}\")\n",
    "        print(f\"Standard deviation: {ratings.std():.2f}\")\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.hist(ratings, bins=50, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Rating')\n",
    "        plt.ylabel('Number of Books')\n",
    "        plt.title('Distribution of Book Ratings')\n",
    "        plt.axvline(ratings.mean(), color='red', linestyle='--', label=f'Mean: {ratings.mean():.2f}')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.boxplot(ratings)\n",
    "        plt.ylabel('Rating')\n",
    "        plt.title('Box Plot of Ratings')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        # Rating categories\n",
    "        rating_categories = pd.cut(ratings, bins=[0, 2, 3, 4, 5], labels=['Poor', 'Fair', 'Good', 'Excellent'])\n",
    "        category_counts = rating_categories.value_counts()\n",
    "        plt.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')\n",
    "        plt.title('Rating Categories')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Correlation with publication year\n",
    "        if 'publication_year' in df.columns:\n",
    "            pub_years = pd.to_numeric(df['publication_year'], errors='coerce')\n",
    "            valid_data = df[(ratings.notna()) & (pub_years.notna()) & (pub_years >= 1900) & (pub_years <= 2025)]\n",
    "            \n",
    "            if len(valid_data) > 0:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.scatter(valid_data['publication_year'], valid_data['rating'], alpha=0.5)\n",
    "                plt.xlabel('Publication Year')\n",
    "                plt.ylabel('Rating')\n",
    "                plt.title('Rating vs Publication Year')\n",
    "                \n",
    "                # Add trend line\n",
    "                z = np.polyfit(valid_data['publication_year'], valid_data['rating'], 1)\n",
    "                p = np.poly1d(z)\n",
    "                plt.plot(valid_data['publication_year'], p(valid_data['publication_year']), \"r--\", alpha=0.8)\n",
    "                \n",
    "                correlation = valid_data['publication_year'].corr(valid_data['rating'])\n",
    "                plt.text(0.05, 0.95, f'Correlation: {correlation:.3f}', transform=plt.gca().transAxes)\n",
    "                \n",
    "                plt.show()\n",
    "else:\n",
    "    print(\"No rating information available in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ef8720",
   "metadata": {},
   "source": [
    "## 9. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa59df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATASET SUMMARY AND INSIGHTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"üìö Total Books: {len(df):,}\")\n",
    "    \n",
    "    if 'author' in df.columns:\n",
    "        unique_authors = df['author'].nunique()\n",
    "        print(f\"‚úçÔ∏è  Unique Authors: {unique_authors:,}\")\n",
    "        print(f\"üìñ Average Books per Author: {len(df)/unique_authors:.1f}\")\n",
    "    \n",
    "    if 'publication_year' in df.columns:\n",
    "        years = pd.to_numeric(df['publication_year'], errors='coerce').dropna()\n",
    "        if len(years) > 0:\n",
    "            print(f\"üìÖ Publication Years: {years.min():.0f} - {years.max():.0f}\")\n",
    "            recent_books = (years >= 2000).sum()\n",
    "            print(f\"üÜï Books from 2000+: {recent_books:,} ({recent_books/len(years)*100:.1f}%)\")\n",
    "    \n",
    "    if 'genre' in df.columns:\n",
    "        genres_with_data = df['genre'].notna().sum()\n",
    "        print(f\"üè∑Ô∏è  Books with Genre Info: {genres_with_data:,} ({genres_with_data/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    if 'description' in df.columns:\n",
    "        desc_with_data = df['description'].notna().sum()\n",
    "        print(f\"üìù Books with Descriptions: {desc_with_data:,} ({desc_with_data/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        avg_desc_length = df['description'].dropna().astype(str).str.len().mean()\n",
    "        print(f\"üìè Average Description Length: {avg_desc_length:.0f} characters\")\n",
    "    \n",
    "    if 'rating' in df.columns:\n",
    "        ratings = pd.to_numeric(df['rating'], errors='coerce').dropna()\n",
    "        if len(ratings) > 0:\n",
    "            print(f\"‚≠ê Average Rating: {ratings.mean():.2f}/5.0\")\n",
    "            print(f\"üéØ Books with Ratings: {len(ratings):,} ({len(ratings)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"RECOMMENDATIONS FOR MODEL TRAINING:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if 'description' in df.columns:\n",
    "        no_desc = df['description'].isna().sum()\n",
    "        if no_desc > 0:\n",
    "            print(f\"‚ö†Ô∏è  {no_desc:,} books lack descriptions - consider data augmentation\")\n",
    "    \n",
    "    if 'genre' in df.columns:\n",
    "        no_genre = df['genre'].isna().sum()\n",
    "        if no_genre > 0:\n",
    "            print(f\"‚ö†Ô∏è  {no_genre:,} books lack genre information - impacts tag prediction\")\n",
    "    \n",
    "    print(\"‚úÖ Dataset appears suitable for semantic model training\")\n",
    "    print(\"‚úÖ Good coverage of books across different time periods\")\n",
    "    print(\"‚úÖ Sufficient text data for embedding generation\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data available for analysis. Please run data collection first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62460094",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on this exploration:\n",
    "\n",
    "1. **Data Quality**: Address missing descriptions and genre information\n",
    "2. **Text Preprocessing**: Clean and standardize text fields\n",
    "3. **Feature Engineering**: Create combined text features for training\n",
    "4. **Tag Vocabulary**: Build comprehensive tag vocabulary from genres\n",
    "5. **Model Training**: Use insights to configure model hyperparameters\n",
    "\n",
    "The dataset shows good potential for training a semantic book recommendation model!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
